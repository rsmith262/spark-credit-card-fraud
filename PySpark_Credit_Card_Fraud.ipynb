{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Credit Card Fraud Analysis Using Spark MLlib</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Credit card fraud increased by 10 percent from 2020 - 2021 and an estimated loss of over 30 Billion dollars [[1]](https://www.statista.com/statistics/1394119/global-card-fraud-losses/). The convenience of debit and credit cards has pushed cash payments into second place in consumer preference. A study in 2021 showed that 57% of payments in the UK were done via card payment with 32% of payments being contactless [[2]](https://www.ukfinance.org.uk/system/files/2022-08/UKF%20Payment%20Markets%20Summary%202022.pdf). Credit card fraud occurs when unauthorised access is gained to an individuals card to make purchases or other transactions [[3]](https://en.wikipedia.org/wiki/Credit_card_fraud). In 2021 it was identified as the second most common form of identity theft with 389,845 complaints [[4]](https://www.experian.com/blogs/ask-experian/identity-theft-statistics/).\n",
    "\n",
    "Due to these alarming statistics and the magnitude of losses being incurred by credit card companies and merchants, there is an ever increasing need to be able to identify fraudulent transactions and stop them before they can go through. Historically, rule based fraud detection has been used, this took a pre programmed set of rules and used these to identify changes in behaviour that could be a sign of credit card fraud [[5]](https://fraud.net/d/rules-based-fraud-detection/). Due to the increases and variety of transactions and the increased sophistication of fraudsters these historic identity systems are no longer viable and in recent years machine learning and AI has increasingly been used in the fight against credit card fraud.\n",
    "\n",
    "Fraud detection presents itself as a binary classification problem. Machine learning is used in credit card fraud detection by taking transactional datapoints and training models to detect the patterns in the data that can be used to identify fraudulent transactions. Generally supervised methods are used that train models on labelled datasets. These models can then be used on new unseen data to predict whether the transaction is real or fraudulent.\n",
    "\n",
    "For this coursework I will analysis a credit card fraud data set to find a model suitable for fraud detection. This is a relevant subject to use due to the harm and financial damage it causes on both a personal level for victims and the wider global impact it has for companies and financial markets. This topic is suitable for this module as credit card fraud is a big data problem. Training the models involve a huge amount of data that will need to be analysed using big data methodologies such as parallel processing frameworks and distributed computing clusters. Due to the complexity of the subject, these models will need continuous tuning so the algorithms stay performing well on new data. The methodologies used in real time detection is also a big data problem with both the model training and real time fraud prediction both involving huge amounts of data and conforming to the typical 5 V's of big data [[6]](https://www.techtarget.com/searchdatamanagement/definition/5-Vs-of-big-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "\n",
    "The data set I have used for my analysis comes from Kaggle and is called 'Credit Card Transactions Fraud Detection Dataset' [[7]](https://www.kaggle.com/datasets/kartik2112/fraud-detection). It is a simulated data set of credit card transactions for 1000 customers with all transactions taking place over the duration of 2019. On Kaggle the data set is split into a train and test data set, however for my analysis I have combined these whilst exploring and creating new features. \n",
    "\n",
    "In total the combined data set has 1,852,394 records and is c500mb in total. And although this is not particular big in the sense of big data my analysis and transformations will all be done using spark and big data methodologies that could be used on much larger data sets. From exploring the data in my EDA below, I found there are 10 numerical columns and 12 categorical. There are no nulls in the data set to deal with or duplicate records.\n",
    "\n",
    "Predicting fraud presents and binary classification problem and due to the nature of fraud analysis the data set is highly imbalanced. There are 9,651 fraudulent records and 1,842,743 records labelled as not fraud. This means only 0.52% of the data is the fraud class. This brings a set of challenges to the modelling as training a model on imbalanced records can cause it to be biased towards the majority.\n",
    "\n",
    "From my initial exploration the 'amt' column stands out as being one to explore - this is the transactional amount. The amounts are highly variable which is seen due to the standard deviation being much higher than the mean. The majority of the transactions are low with larger outliers causing the mean to be significantly higher than the median.\n",
    "\n",
    "The data from this shows the median is lower than the mean because of high value outliers. It shows that 25% of the transactions are lower than or equal to 9.65 which shows a very large number of transactions are small. And 75% of the transactions are lower or equal to 83.09. My takeaway from this is the amount is highly variable and right skewed with the majority of the transactions being relatively low but a few high value transactions that push up the mean and standard deviation.\n",
    "\n",
    "This is something I will be investigating later in conjunction with target variable as I would hypothesise the fraud transactions are likely to be smaller in value as high value transactions are more likely to be scrutinised.\n",
    "\n",
    "Age is another variable I would like to explore further as part of my initial research into the subject suggests certain age groups are more susceptible to fraud. With the 30-39 age group being most likely [[8]](https://www.bankrate.com/finance/credit-cards/credit-card-fraud-statistics/#fraud),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table showing summary statistics of the numerical variables in the data set\n",
    "\n",
    "| summary | _c0       | amt      | zip      | lat      | long     | city_pop  | unix_time    | merch_lat | merch_long | is_fraud |\n",
    "|---------|-----------|----------|----------|----------|----------|-----------|--------------|-----------|------------|----------|\n",
    "| count   | 1852394.0 | 1852394.0| 1852394.0| 1852394.0| 1852394.0| 1852394.0 | 1852394.0    | 1852394.0 | 1852394.0  |1852394.0 |\n",
    "| mean    | 537193.44 | 70.06    | 48813.26 | 38.54    | -90.23   | 88643.67  | 1.35867418E9 | 38.54     | -90.23     | 0.01     |\n",
    "| stddev  | 366910.97 | 159.25   | 26881.85 | 5.07     | 13.75    | 301487.62 | 1.8195082E7  | 5.11      | 13.76      | 0.07     |\n",
    "| min     | 0.0       | 1.0      | 1257.0   | 20.03    | -165.67  | 23.0      | 1.325376E9   | 19.03     | -166.67    | 0.0      |\n",
    "| max     | 1296674.0 | 28948.9  | 99921.0  | 66.69    | -67.95   | 2906700.0 | 1.3885344E9  | 67.51     | -66.95     | 1.0      |\n",
    "\n",
    "\n",
    "This shows 'amt' in more detail\n",
    "\n",
    "| summary |               amt |\n",
    "| ------- | ----------------- |\n",
    "| count   |           1852394 |\n",
    "| mean    | 70.06356747538618 |\n",
    "| stddev  |  159.253974773983 |\n",
    "| min     |               1.0 |\n",
    "| 25%     |              9.65 |\n",
    "| 50%     |             47.45 |\n",
    "| 75%     |             83.09 |\n",
    "| max     |           28948.9 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my analysis performs cross validation and gridsearch for hyperparameter tuning, the notebook takes a long time to run, therefore I have provided a very small sample set for marking purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset from Kaggle\n",
    "\n",
    "https://www.kaggle.com/datasets/kartik2112/fraud-detection\n",
    "\n",
    "#### Data sets stored in the following locations on hdfs\n",
    "\n",
    "#### Full Data\n",
    "hdfs:///user/rsmit001/CW2/fraudFiles/fraudTrain.csv\n",
    "\n",
    "hdfs:///user/rsmit001/CW2/fraudFiles/fraudTrain.csv\n",
    "\n",
    "#### Sample Data\n",
    "hdfs:///user/rsmit001/CW2/fraudFiles/SAMPLE_fraudTrain_v2/part-00000-ce7b1ce2-c467-4c10-9451-31ed680a62c5-c000.csv\n",
    "\n",
    "hdfs:///user/rsmit001/CW2/fraudFiles/SAMPLE_fraudTest_v2/part-00000-9c8ab8f3-8fb6-4fac-be83-528e13a62647-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses and Empirical Tests\n",
    "\n",
    "Forming part of my analysis I will be running a number of empirical tasks and testing hypotheses:\n",
    "\n",
    "1. Sampling methods\n",
    "\n",
    "To train an effective model I will need to apply sampling methods to balance out the datasets. I propose to test models where I have oversampled the data set and undersampled. Oversampling keeps all of the majority class and replicates records from the minority class to create a balanced data set. This prevents data loss because all original records are kept, however it does introduce bias as the minority records are repeated multiple times. Undersampling is the opposite and is done by deleting enough of the majority class to balance the data set that way, this though will cause a large amount of valuable data to be lost. I hypothesise that oversampling will produce better results as there will be a noteable amount more data to train the model with.\n",
    "\n",
    "2. Testing different models\n",
    "\n",
    "I propose to test 3 different models to see which performs best:\n",
    "\n",
    "* Logistic regression\n",
    "* Naive Bayes\n",
    "* Random Forests\n",
    "\n",
    "I hypothesise the best results will be achieved via a Random Forest model. The random forest algorithm is an ensemble method that uses bootstrapping methodology and creates multiple decision trees from the data and the averages the results to produce powerful predictions [[9]](https://towardsdatascience.com/random-forest-classification-678e551462f5). A benchmarking study in 2019 found that Random Forests performed better than logistic regression on 69% of the data sets tested [[10]](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2264-5)\n",
    "\n",
    "3. Cross Validation and Gridsearch\n",
    "\n",
    "I propose to use cross validation (CV) as part of the training pipeline. CV splits the data into k-folds so you can effecitvely use all the training data to train and test the models. This helps it to prevent overfitting and means the algorithms should generalise to new data more effectively.\n",
    "\n",
    "Alongside CV I will use gridsearch to find the optimal hyperparameters to be used to train the final model. Gridsearch takes a given grid of hyperparameters and iterates through them performing CV on each combination. It then evaluates each model and gives the best set of hyperparameters as its final best model output.\n",
    "\n",
    "4. Evaluation\n",
    "\n",
    "After I've output the best model this will be used on the hold out test set to predict the target class. I will evaluate it's performance using a number of metrics. Due to the data being imbalanced certain methods such as accuracy will be misleading as you could predict all labels as not fraud and still get a 99.5% accuracy score. Evaluation methods such as AUC-ROC and AUC-PR will give a better sense of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planned Analysis\n",
    "\n",
    "I plan to do all my analysis and coding using Pyspark. The data will be stored in HDFS and will use YARN for processing. I will be using Spark MLlib for model algorithms, pipeline tools and evaluation. This is my proposed pipeline:\n",
    "\n",
    "* Exploratory data analysis\n",
    "* Data cleaning and feature engineering\n",
    "* Drop unnecessary columns\n",
    "* Under/oversample the data \n",
    "* Build an ML Pipeline\n",
    "    * One hot encoding\n",
    "    * Transforming the data into a single vector for input into the model\n",
    "    * scaling the data\n",
    "    * CV/Gridearch\n",
    "* Evaluate the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession, Row # imports SparkSession\n",
    "import pyspark.sql.functions as F # functions for working with df's\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType # to help inspect the schema\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('CCFraudAnalysis') \\\n",
    "        .master('yarn') \\\n",
    "        .config('spark.executor.memory', '4g') \\\n",
    "        .config('spark.executor.cores', '4') \\\n",
    "        .config('spark.cores.max', '4') \\\n",
    "        .config('spark.driver.memory','4g') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lena-master:4069\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CCFraudAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f608071d370>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks session is open\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to marker:\n",
    "Please change the code below to run on the sample sets included with my submission. It may still take a little while to run due to the gridsearch cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "fraud_train = spark.read.csv('hdfs:///user/rsmit001/CW2/fraudFiles/fraudTrain.csv',header=True,inferSchema=True)\n",
    "fraud_test = spark.read.csv('hdfs:///user/rsmit001/CW2/fraudFiles/fraudTest.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below makes a sample of the full data sets to provide with my coursework submission so it can be run and tested by the markers. I've given a very small sample as the model training is time consuming due to gridsearch cross validation. This code is commented out as does not need to run each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating sample, commenting out so it doesnt run when each time the notebook is run\n",
    "# # used sampleby which returns a stratified sample to keep the ratio of fraud/not fraud\n",
    "# # https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.sampleBy.html\n",
    "# fractions = {0:0.001,1:0.001} # creates fraction of each to split by - I want even split\n",
    "# stratified_fraud_train = fraud_train.sampleBy(\"is_fraud\", fractions, seed=42)\n",
    "# stratified_fraud_test = fraud_test.sampleBy(\"is_fraud\", fractions, seed=42)\n",
    "\n",
    "# # validation, checks the target class stays in the correct ratio\n",
    "# print(stratified_fraud_train.filter(stratified_fraud_train.is_fraud==0).count()/stratified_fraud_train.count())\n",
    "# print(stratified_fraud_test.filter(stratified_fraud_test.is_fraud==0).count()/stratified_fraud_test.count())\n",
    "\n",
    "# # write to hdfs\n",
    "# stratified_fraud_train.coalesce(1).write.csv('hdfs:///user/rsmit001/CW2/fraudFiles/'\n",
    "#                                              'SAMPLE_fraudTrain_v2', header=True)\n",
    "# stratified_fraud_test.coalesce(1).write.csv('hdfs:///user/rsmit001/CW2/fraudFiles/'\n",
    "#                                             'SAMPLE_fraudTest_v2', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False to run on full set, True to run on sample\n",
    "SAMPLE = False\n",
    "if SAMPLE:\n",
    "    fraud_train = spark.read.csv('hdfs:///user/rsmit001/CW2/fraudFiles/SAMPLE_fraudTrain_v2/part-00000-ce7b1ce2'\n",
    "                                 '-c467-4c10-9451-31ed680a62c5-c000.csv',header=True,inferSchema=True)\n",
    "    fraud_test = spark.read.csv('hdfs:///user/rsmit001/CW2/fraudFiles/SAMPLE_fraudTest_v2/part-00000-9c8ab8f3-8fb6'\n",
    "                                '-4fac-be83-528e13a62647-c000.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data checks, exploration and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to do a couple of basic checks on the data sets to make sure the data sets are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------\n",
      " _c0                   | 0                                \n",
      " trans_date_trans_time | 2019-01-01 00:00:18              \n",
      " cc_num                | 2703186189652095                 \n",
      " merchant              | fraud_Rippin, Kub and Mann       \n",
      " category              | misc_net                         \n",
      " amt                   | 4.97                             \n",
      " first                 | Jennifer                         \n",
      " last                  | Banks                            \n",
      " gender                | F                                \n",
      " street                | 561 Perry Cove                   \n",
      " city                  | Moravian Falls                   \n",
      " state                 | NC                               \n",
      " zip                   | 28654                            \n",
      " lat                   | 36.0788                          \n",
      " long                  | -81.1781                         \n",
      " city_pop              | 3495                             \n",
      " job                   | Psychologist, counselling        \n",
      " dob                   | 1988-03-09                       \n",
      " trans_num             | 0b242abb623afc578575680df30655b9 \n",
      " unix_time             | 1325376018                       \n",
      " merch_lat             | 36.011293                        \n",
      " merch_long            | -82.048315                       \n",
      " is_fraud              | 0                                \n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# showing vertical as data looks very untidy horizontal\n",
    "print(fraud_train.show(n=1,truncate=False,vertical=True)) # prints 1 record from fraud train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------\n",
      " _c0                   | 0                                \n",
      " trans_date_trans_time | 2020-06-21 12:14:25              \n",
      " cc_num                | 2291163933867244                 \n",
      " merchant              | fraud_Kirlin and Sons            \n",
      " category              | personal_care                    \n",
      " amt                   | 2.86                             \n",
      " first                 | Jeff                             \n",
      " last                  | Elliott                          \n",
      " gender                | M                                \n",
      " street                | 351 Darlene Green                \n",
      " city                  | Columbia                         \n",
      " state                 | SC                               \n",
      " zip                   | 29209                            \n",
      " lat                   | 33.9659                          \n",
      " long                  | -80.9355                         \n",
      " city_pop              | 333497                           \n",
      " job                   | Mechanical engineer              \n",
      " dob                   | 1968-03-19                       \n",
      " trans_num             | 2da90c7d74bd46a0caf3777415b3ebd3 \n",
      " unix_time             | 1371816865                       \n",
      " merch_lat             | 33.986391                        \n",
      " merch_long            | -81.200714                       \n",
      " is_fraud              | 0                                \n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fraud_test.show(n=1,truncate=False,vertical=True)) # prints 1 record from fraud test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n",
      "None\n",
      "\n",
      "Both dataframes have the same schema.\n"
     ]
    }
   ],
   "source": [
    "# print the schemas to check they're the same\n",
    "print(fraud_train.printSchema())\n",
    "print(fraud_test.printSchema())\n",
    "\n",
    "# Compare the schemas\n",
    "if fraud_train.schema == fraud_test.schema:\n",
    "    print('\\nBoth dataframes have the same schema.')\n",
    "else:\n",
    "    print('\\nThe dataframes have different schemas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 1296675 records\n",
      "The test set has 555719 records\n"
     ]
    }
   ],
   "source": [
    "# check how many records are in the datasets\n",
    "print(f'The training set has {fraud_train.count()} records')\n",
    "print(f'The test set has {fraud_test.count()} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we could discard the test data set as we already have nearly 1.3 million records to work with. However, the test set has another 555k records to inform the models so I believe the best route would be to combine these to do data cleaning then resplit later. Having them split at the point isn't really that helpful as both data sets would need to be cleaned which would add work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate both data sets using union\n",
    "concat_data = fraud_train.union(fraud_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concatenated set has 1852394 records\n"
     ]
    }
   ],
   "source": [
    "# check the new data set\n",
    "print(f'The concatenated set has {concat_data.count()} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframes have the same schema.\n"
     ]
    }
   ],
   "source": [
    "# quick check of the schema again to make sure everything is as expected\n",
    "# Compare the schemas\n",
    "if concat_data.schema == fraud_train.schema == fraud_test.schema:\n",
    "    print('\\nThe dataframes have the same schema.')\n",
    "else:\n",
    "    print('\\nThe dataframes have different schemas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been combined succesfully ready for preprocessing. From looking at the schema a couple of variables jump straight out for looking into later, 'trans_date_trans_time' and 'dob'. These are both strings and I would expect date time to be the most usable format for these. I will look into these further later on.\n",
    "\n",
    "First I'm going to check for NaN values and duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n",
      " _c0                   | 0   \n",
      " trans_date_trans_time | 0   \n",
      " cc_num                | 0   \n",
      " merchant              | 0   \n",
      " category              | 0   \n",
      " amt                   | 0   \n",
      " first                 | 0   \n",
      " last                  | 0   \n",
      " gender                | 0   \n",
      " street                | 0   \n",
      " city                  | 0   \n",
      " state                 | 0   \n",
      " zip                   | 0   \n",
      " lat                   | 0   \n",
      " long                  | 0   \n",
      " city_pop              | 0   \n",
      " job                   | 0   \n",
      " dob                   | 0   \n",
      " trans_num             | 0   \n",
      " unix_time             | 0   \n",
      " merch_lat             | 0   \n",
      " merch_long            | 0   \n",
      " is_fraud              | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for Nan's\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "concat_data.select([F.count(F.when(F.col(c).isNull() , c)).alias(c) for c in concat_data.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct rows: 1852394\n",
      "Number of distinct rows: 1852394\n",
      "Number of distinct rows: 0\n"
     ]
    }
   ],
   "source": [
    "# check the data set for duplicate rows\n",
    "distinct_count = concat_data.distinct().count()\n",
    "total_count = concat_data.count()\n",
    "duplicate_count = distinct_count - total_count\n",
    "print(f'Number of distinct rows: {distinct_count}')\n",
    "print(f'Number of distinct rows: {total_count}')\n",
    "print(f'Number of distinct rows: {duplicate_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate values to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the variables in more detail, I'll be looking at some basic statistics and see if there are any outliers in the numerical variables or dirty data in the categoricals. I used geeksforgeeks [[11]](https://www.geeksforgeeks.org/selecting-only-numeric-or-string-columns-names-from-pyspark-dataframe/) for reference on using the .schema method to create lists of numeric and categoric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: 10\n",
      "Categorical columns: 12\n"
     ]
    }
   ],
   "source": [
    "# first to get a list of numeric and categorical variables\n",
    "# https://www.geeksforgeeks.org/selecting-only-numeric-or-string-columns-names-from-pyspark-dataframe/\n",
    "\n",
    "# assigning empty list\n",
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "schema = concat_data.schema # gets df schema\n",
    "\n",
    "# update lists based on the data types of the columns (from schema)\n",
    "for var in schema.fields:\n",
    "    if isinstance(var.dataType, (IntegerType, DoubleType)):\n",
    "        num_cols.append(var.name)\n",
    "    elif isinstance(var.dataType, StringType):\n",
    "        cat_cols.append(var.name)\n",
    "\n",
    "print(f'Numerical columns: {len(num_cols)}')\n",
    "print(f'Categorical columns: {len(cat_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns:\n",
      ":['_c0', 'amt', 'zip', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud']\n",
      "\n",
      "Categorical columns:\n",
      ":['trans_date_trans_time', 'merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'dob', 'trans_num']\n"
     ]
    }
   ],
   "source": [
    "print(f'Numerical columns:\\n:{num_cols}\\n')\n",
    "print(f'Categorical columns:\\n:{cat_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+---------+---------+---------+------------+---------+----------+---------+\n",
      "|summary|      _c0|      amt|      zip|      lat|     long| city_pop|   unix_time|merch_lat|merch_long| is_fraud|\n",
      "+-------+---------+---------+---------+---------+---------+---------+------------+---------+----------+---------+\n",
      "|  count|1852394.0|1852394.0|1852394.0|1852394.0|1852394.0|1852394.0|   1852394.0|1852394.0| 1852394.0|1852394.0|\n",
      "|   mean|537193.44|    70.06| 48813.26|    38.54|   -90.23| 88643.67|1.35867418E9|    38.54|    -90.23|     0.01|\n",
      "| stddev|366910.97|   159.25| 26881.85|     5.07|    13.75|301487.62| 1.8195082E7|     5.11|     13.76|     0.07|\n",
      "|    min|      0.0|      1.0|   1257.0|    20.03|  -165.67|     23.0|  1.325376E9|    19.03|   -166.67|      0.0|\n",
      "|    max|1296674.0|  28948.9|  99921.0|    66.69|   -67.95|2906700.0| 1.3885344E9|    67.51|    -66.95|      1.0|\n",
      "+-------+---------+---------+---------+---------+---------+---------+------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# basic statistics of the numerical values\n",
    "\n",
    "# Run describe to get summary statistics for numerical columns\n",
    "desc_df = concat_data.describe(num_cols)\n",
    "\n",
    "# Round each of the numerical statistics to 2 decimal places - this is so the summary df is displayed\n",
    "# in a readable form\n",
    "for col_name in num_cols:\n",
    "    desc_df = desc_df.withColumn(col_name, F.round(F.col(col_name).cast('float'), 2))\n",
    "    \n",
    "desc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main variable that stands out to me here is 'amt' which is the transaction amount. With a mean transaction amount of 70.06 but min and max of 1 and 28,948.90 respectively. This shows there are big differences in the value of the transactions being made and with a standard deviation of 159.25 it shows the data has high variability. To investigate this a bit further I'll run the df.summary() function as this includes quartiles too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               amt|\n",
      "+-------+------------------+\n",
      "|  count|           1852394|\n",
      "|   mean| 70.06356747538561|\n",
      "| stddev|159.25397477398326|\n",
      "|    min|               1.0|\n",
      "|    25%|              9.65|\n",
      "|    50%|             47.45|\n",
      "|    75%|             83.09|\n",
      "|    max|           28948.9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat_data.select('amt').summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical data shows the median is lower than the mean due to the effects of high value outliers. It shows that 25% of the transactions are lower than or equal to 9.65 which means a large amount of transactions are small in value. And 75% of the transactions are lower or equal to 83.09. My takeaway from this is the amount is highly variable and right skewed with the majority of the transactions being relatively low but a few high value transactions that push up the mean and standard deviation.\n",
    "\n",
    "This is something I will be investigating later in conjunction with the target variable as I would hypothesise the fraud transactions are likely to be smaller in value as high value transactions are more likely to be scrutinised.\n",
    "\n",
    "I'll have a quick check of the categorical variables and see how many unique values there are before delving into analysis of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique values in variable \"trans_date_trans_time\": 1819551\n",
      "number of unique values in variable \"merchant\": 693\n",
      "number of unique values in variable \"category\": 14\n",
      "number of unique values in variable \"first\": 355\n",
      "number of unique values in variable \"last\": 486\n",
      "number of unique values in variable \"gender\": 2\n",
      "number of unique values in variable \"street\": 999\n",
      "number of unique values in variable \"city\": 906\n",
      "number of unique values in variable \"state\": 51\n",
      "number of unique values in variable \"job\": 497\n",
      "number of unique values in variable \"dob\": 984\n",
      "number of unique values in variable \"trans_num\": 1852394\n"
     ]
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    print(f'number of unique values in variable \"{col}\": {concat_data.select(col).distinct().count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables that stand out as being woth looking at further are:\n",
    "\n",
    "* 'merchant' - are there particular merchants with lax security that fraudsters target?\n",
    "* 'category' - are certain categories of transaction more likely to fraudulant?\n",
    "* 'gender' - is this a factor? Are people of one gender more likely to be targeted \n",
    "* 'city'/'state' - is location a factor\n",
    "* 'job' - can fraud be corralated with job role. Are certain people more likely to be targeted\n",
    "\n",
    "* 'dob' - I'll look at this further after creating an age variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the target class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've created a function to output this summary as I've repeated it a few times below\n",
    "def fraud_not_fraud_summary(df):\n",
    "    \"\"\"\n",
    "    takes a dataframe as input and calculates the number of class 1 (fraud), class 0 (not fraud) records\n",
    "    and the percentage not fraud to fraud\n",
    "    \n",
    "    args:\n",
    "        df (dataframe): the data frame\n",
    "    \n",
    "    returns:\n",
    "        it prints the number of class 1 (fraud), class 0 (not fraud) records\n",
    "        and the percentage not fraud to fraud\n",
    "    \n",
    "    \"\"\"\n",
    "    total_records = df.count()\n",
    "    fraud_total = df.filter(df.is_fraud==1).count() # total records that are fraud\n",
    "    not_fraud_total = df.filter(df.is_fraud==0).count() # total records that are NOT fraud\n",
    "\n",
    "    print(f'Number of class 1 (fraudulant records): {fraud_total}')\n",
    "    print(f'Number of class 0 (Non-fraudulant records): {not_fraud_total}')\n",
    "    print(f'{float(not_fraud_total)/float(total_records)*100}% are in class 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class 1 (fraudulant records): 9651\n",
      "Number of class 0 (Non-fraudulant records): 1842743\n",
      "99.47899852839083% are in class 0\n"
     ]
    }
   ],
   "source": [
    "fraud_not_fraud_summary(concat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 99.5% of the records being not fraud the data set is heavily imbalanced. This will need to be dealt with later on or the model will overfit when training. I will need to apply under or oversampling to balance the classes out. I'll discuss this in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now look at how the classes are distributed in comparison to the other variables. I would like to see how the target variable is distributed and see of I can see any initial trends that could highlight key variables for the model. I'll look at the categorical variables highlighted above first. My technique will be to normalise the data by creating a within category percentage and sorting on this. This will take into account categories that have values with large numbers of transactions by getting a percentage based on total fraud transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add docstring\n",
    "# function to print top 10 categories with highest % of fraud\n",
    "def percentage_fraud_cat(df,column,n):\n",
    "    \"\"\"\n",
    "    takes a dataframe as input and for a given column prints top 10 categories with highest % of fraud\n",
    "    \n",
    "    args:\n",
    "        df (dataframe): the dataframe\n",
    "        column: (dataframe column) specific column to do calculations on\n",
    "        n (int): number of within feature values to summarise on (e.g. top 10 values)\n",
    "        \n",
    "    \n",
    "    returns:\n",
    "        it displays the resulting dataframe sorted by 'within_category_fraud_%' and top values\n",
    "    \n",
    "    \"\"\"\n",
    "    # creates df grouped by categorical variable, totals per category then renames col\n",
    "    all_df = df.groupBy(column).count().withColumnRenamed('count', 'total_count')\n",
    "\n",
    "    # creates df similar to above but filtered to NON fraud cases\n",
    "    non_fraud_df = df.filter(df.is_fraud == 0).groupBy(column).count()\\\n",
    "    .withColumnRenamed('count', 'non_fraud_count')\n",
    "\n",
    "    # creates df similar to above but filtered to fraud cases\n",
    "    fraud_df = df.filter(df.is_fraud == 1).groupBy(column).count()\\\n",
    "    .withColumnRenamed('count', 'fraud_count')\n",
    "\n",
    "    # joins all df's based on categorical col\n",
    "    joined_df = all_df.join(non_fraud_df, column, 'outer').join(fraud_df, column, 'outer').fillna(0)\n",
    "\n",
    "    # calculate within cat fraud % and rounds\n",
    "    result_df = joined_df.withColumn\\\n",
    "    ('within_category_fraud_%', F.round((F.col('fraud_count') / F.col('total_count') * 100), 2))\n",
    "\n",
    "    # sorts\n",
    "    result_df = result_df.orderBy(F.desc('within_category_fraud_%'))\n",
    "\n",
    "    # shows top n\n",
    "    result_df.show(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I havent applied this to all categorical columns as first and last names and transaction number and date will not give any meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "|            merchant|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "|   fraud_Kozey-Boehm|       2758|           2698|         60|                   2.18|\n",
      "|fraud_Herman, Tre...|       1870|           1832|         38|                   2.03|\n",
      "|    fraud_Terry-Huel|       2864|           2808|         56|                   1.96|\n",
      "|fraud_Kerluke-Abs...|       2635|           2585|         50|                    1.9|\n",
      "|fraud_Mosciski, Z...|       2821|           2768|         53|                   1.88|\n",
      "|fraud_Schmeler, B...|       2788|           2736|         52|                   1.87|\n",
      "|     fraud_Kuhic LLC|       2842|           2789|         53|                   1.86|\n",
      "|      fraud_Jast Ltd|       2757|           2706|         51|                   1.85|\n",
      "|fraud_Langworth, ...|       2817|           2765|         52|                   1.85|\n",
      "|fraud_Romaguera, ...|       2767|           2716|         51|                   1.84|\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Category\n",
      "+-------------+-----------+---------------+-----------+-----------------------+\n",
      "|     category|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+-------------+-----------+---------------+-----------+-----------------------+\n",
      "| shopping_net|     139322|         137103|       2219|                   1.59|\n",
      "|     misc_net|      90654|          89472|       1182|                    1.3|\n",
      "|  grocery_pos|     176191|         173963|       2228|                   1.26|\n",
      "| shopping_pos|     166463|         165407|       1056|                   0.63|\n",
      "|gas_transport|     188029|         187257|        772|                   0.41|\n",
      "|     misc_pos|     114229|         113907|        322|                   0.28|\n",
      "|       travel|      57956|          57800|        156|                   0.27|\n",
      "|  grocery_net|      64878|          64703|        175|                   0.27|\n",
      "|entertainment|     134118|         133826|        292|                   0.22|\n",
      "|personal_care|     130085|         129795|        290|                   0.22|\n",
      "+-------------+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Gender\n",
      "+------+-----------+---------------+-----------+-----------------------+\n",
      "|gender|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+------+-----------+---------------+-----------+-----------------------+\n",
      "|     M|     837645|         832893|       4752|                   0.57|\n",
      "|     F|    1014749|        1009850|       4899|                   0.48|\n",
      "+------+-----------+---------------+-----------+-----------------------+\n",
      "\n",
      "City\n",
      "+-----------+-----------+---------------+-----------+-----------------------+\n",
      "|       city|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+-----------+-----------+---------------+-----------+-----------------------+\n",
      "|   Streator|          7|              0|          7|                  100.0|\n",
      "|     Crouse|          8|              0|          8|                  100.0|\n",
      "|     Roland|         11|              0|         11|                  100.0|\n",
      "|      Craig|         14|              0|         14|                  100.0|\n",
      "|Springville|         12|              0|         12|                  100.0|\n",
      "|     Angwin|         10|              0|         10|                  100.0|\n",
      "| East China|          9|              0|          9|                  100.0|\n",
      "|Orange Park|         10|              0|         10|                  100.0|\n",
      "|Chattanooga|          7|              0|          7|                  100.0|\n",
      "| Brookfield|          9|              0|          9|                  100.0|\n",
      "+-----------+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "State\n",
      "+-----+-----------+---------------+-----------+-----------------------+\n",
      "|state|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+-----+-----------+---------------+-----------+-----------------------+\n",
      "|   DE|          9|              0|          9|                  100.0|\n",
      "|   RI|        745|            730|         15|                   2.01|\n",
      "|   AK|       2963|           2913|         50|                   1.69|\n",
      "|   OR|      26408|          26211|        197|                   0.75|\n",
      "|   NH|      11727|          11648|         79|                   0.67|\n",
      "|   VA|      41756|          41483|        273|                   0.65|\n",
      "|   TN|      24913|          24754|        159|                   0.64|\n",
      "|   NE|      34425|          34209|        216|                   0.63|\n",
      "|   MN|      45433|          45153|        280|                   0.62|\n",
      "|   NY|     119419|         118689|        730|                   0.61|\n",
      "+-----+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Job\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "|                 job|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "|Contracting civil...|          7|              0|          7|                  100.0|\n",
      "|   Personnel officer|         12|              0|         12|                  100.0|\n",
      "|Sales promotion a...|         14|              0|         14|                  100.0|\n",
      "|Forest/woodland m...|          9|              0|          9|                  100.0|\n",
      "|Operational inves...|         11|              0|         11|                  100.0|\n",
      "|Air traffic contr...|         17|              0|         17|                  100.0|\n",
      "|         Ship broker|          7|              0|          7|                  100.0|\n",
      "|           Homeopath|         11|              0|         11|                  100.0|\n",
      "|Broadcast journalist|          9|              0|          9|                  100.0|\n",
      "|Armed forces tech...|          8|              0|          8|                  100.0|\n",
      "+--------------------+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applies the function on a subset of the cateorical columns\n",
    "cats = ['merchant','category','gender','city','state','job'] # list of cols to check\n",
    "for i in cats:\n",
    "    print(i.capitalize())\n",
    "    percentage_fraud_cat(concat_data,i,10) # shows top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standout feature from this analysis is transaction category, this shows there are 3 categories with significantly higher percentages of fraud (shopping_net,misc_net,grocery-pos). This looks like a feature that would be beneficial for the model.\n",
    "\n",
    "Gender shows there is a higher chance of fraud if you are male over female but it is not too drastic. Some of the other features such as city and state look like there are within category values that have high fraud but this is due to all transactions within that category being fraud but will not prove to be significant but just chance occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a distance feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thoughts are the distance between the credit card holder and the fraudulant transaction might give an indication of fraud. Would it be more likely stolen details would be used close to the victim or further away? I've created a feature for the distance between the coordinates of the credit card holder and the merchant where the transaction took place. I've used these two websites for reference in creating this feature [[12]](https://en.wikipedia.org/wiki/Haversine_formula) [[13]](https://gist.github.com/pavlov99/bd265be244f8a84e291e96c5656ceb5c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distancew between two coordinates\n",
    "# https://en.wikipedia.org/wiki/Haversine_formula\n",
    "# using code from here https://gist.github.com/pavlov99/bd265be244f8a84e291e96c5656ceb5c\n",
    "\n",
    "concat_data = concat_data.withColumn(\"a\", (\n",
    "    F.pow(F.sin(F.radians(F.col(\"lat\") - F.col(\"merch_lat\")) / 2), 2) +\n",
    "    F.cos(F.radians(F.col(\"merch_lat\"))) * F.cos(F.radians(F.col(\"lat\"))) *\n",
    "    F.pow(F.sin(F.radians(F.col(\"long\") - F.col(\"merch_long\")) / 2), 2)\n",
    ")).withColumn(\"distance\", F.atan2(F.sqrt(F.col(\"a\")), F.sqrt(-F.col(\"a\") + 1)) * 12742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            distance|\n",
      "+-------+--------------------+\n",
      "|  count|             1852394|\n",
      "|   mean|   76.11172606007756|\n",
      "| stddev|  29.116970235829854|\n",
      "|    min|0.022254515638296817|\n",
      "|    25%|   55.31745761253762|\n",
      "|    50%|    78.2134066416527|\n",
      "|    75%|   98.50607848728853|\n",
      "|    max|  152.11717310594932|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all records summary\n",
    "concat_data.select('distance').summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          distance|\n",
      "+-------+------------------+\n",
      "|  count|              9651|\n",
      "|   mean|  76.2562333827283|\n",
      "| stddev|28.865544711903578|\n",
      "|    min|0.7387691216521973|\n",
      "|    25%|144.52241007387727|\n",
      "|    50%| 78.10192247309428|\n",
      "|    75%|144.52241007387727|\n",
      "|    max|144.52241007387727|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fraud records summary\n",
    "concat_data.filter(concat_data.is_fraud == 1).select('distance').\\\n",
    "summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            distance|\n",
      "+-------+--------------------+\n",
      "|  count|             1842743|\n",
      "|   mean|   76.11096923171304|\n",
      "| stddev|  29.118287189373707|\n",
      "|    min|0.022254515638296817|\n",
      "|    25%|   55.31745761253762|\n",
      "|    50%|   78.21634859215874|\n",
      "|    75%|   98.50607848728853|\n",
      "|    max|  152.11717310594932|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not fraud records summary\n",
    "concat_data.filter(concat_data.is_fraud == 0).select('distance').\\\n",
    "summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase there are is such a large imbalance between fraud and not fraud, the means are going to be quite similar, however, the median shows the median fraudulant transaction is much further away than those of non fraud, or the overall median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing transaction time from string to date time and extracting day, hour features. I've used these sources for reference [[14]](https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/) [[15]](https://sparkbyexamples.com/spark/spark-extract-hour-minute-and-second-from-timestamp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp String to DateType\n",
    "# https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/\n",
    "# https://sparkbyexamples.com/spark/spark-extract-hour-minute-and-second-from-timestamp/\n",
    "concat_data = concat_data.withColumn('trans_timestamp',F.to_timestamp('trans_date_trans_time'))\n",
    "\n",
    "# extracting day and hour\n",
    "concat_data = concat_data.withColumn('trans_day', F.date_format('trans_timestamp', 'E'))\\\n",
    ".withColumn(\"trans_hour\", F.hour(F.col(\"trans_timestamp\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploring day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|trans_day|count|\n",
      "+---------+-----+\n",
      "|      Sun| 1590|\n",
      "|      Sat| 1493|\n",
      "|      Mon| 1484|\n",
      "|      Fri| 1376|\n",
      "|      Thu| 1317|\n",
      "|      Tue| 1266|\n",
      "|      Wed| 1125|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat_data.filter(concat_data.is_fraud == 1).groupBy('trans_day').count().orderBy(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+-----------+-----------------------+\n",
      "|trans_day|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+---------+-----------+---------------+-----------+-----------------------+\n",
      "|      Thu|     206741|         205424|       1317|                   0.64|\n",
      "|      Fri|     215078|         213702|       1376|                   0.64|\n",
      "|      Wed|     183913|         182788|       1125|                   0.61|\n",
      "|      Sat|     263227|         261734|       1493|                   0.57|\n",
      "|      Tue|     270340|         269074|       1266|                   0.47|\n",
      "|      Sun|     343677|         342087|       1590|                   0.46|\n",
      "|      Mon|     369418|         367934|       1484|                    0.4|\n",
      "+---------+-----------+---------------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using function from above\n",
    "percentage_fraud_cat(concat_data,'trans_day',7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, fraud is likeliest on Thursday and Friday and least likely on Mondays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring hour of day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|trans_hour|count|\n",
      "+----------+-----+\n",
      "|        22| 2481|\n",
      "|        23| 2442|\n",
      "|         1|  827|\n",
      "|         0|  823|\n",
      "|         3|  803|\n",
      "|         2|  793|\n",
      "|        18|  111|\n",
      "|        19|  105|\n",
      "|        21|  101|\n",
      "|        15|  100|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat_data.filter(concat_data.is_fraud == 1).groupBy('trans_hour').count().orderBy(F.desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-----------+-----------------------+\n",
      "|trans_hour|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+----------+-----------+---------------+-----------+-----------------------+\n",
      "|        22|      95370|          92889|       2481|                    2.6|\n",
      "|        23|      95902|          93460|       2442|                   2.55|\n",
      "|         0|      60655|          59832|        823|                   1.36|\n",
      "|         1|      61330|          60503|        827|                   1.35|\n",
      "|         3|      60968|          60165|        803|                   1.32|\n",
      "|         2|      60796|          60003|        793|                    1.3|\n",
      "|         5|      60088|          60008|         80|                   0.13|\n",
      "|         7|      60301|          60229|         72|                   0.12|\n",
      "|        18|      94052|          93941|        111|                   0.12|\n",
      "|        14|      93089|          92989|        100|                   0.11|\n",
      "|        15|      93439|          93339|        100|                   0.11|\n",
      "|        19|      93433|          93328|        105|                   0.11|\n",
      "|        21|      93738|          93637|        101|                   0.11|\n",
      "|        20|      93081|          92983|         98|                   0.11|\n",
      "|        13|      93492|          93398|         94|                    0.1|\n",
      "|        11|      60170|          60111|         59|                    0.1|\n",
      "|         4|      59938|          59877|         61|                    0.1|\n",
      "|        16|      94289|          94192|         97|                    0.1|\n",
      "|         8|      60498|          60439|         59|                    0.1|\n",
      "|         9|      60231|          60170|         61|                    0.1|\n",
      "|        17|      93514|          93420|         94|                    0.1|\n",
      "|        10|      60320|          60268|         52|                   0.09|\n",
      "|        12|      93294|          93210|         84|                   0.09|\n",
      "|         6|      60406|          60352|         54|                   0.09|\n",
      "+----------+-----------+---------------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_fraud_cat(concat_data,'trans_hour',24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty conclusive, fraud is much more likely to happen between 22:00 and 04:00. To capture this in my data I'm going to create a binary category 'hour_binary' where 1 if between 22:00 and 04:00 and 0 for any other times. I've used this source for reference here [[16]](https://stackoverflow.com/questions/39048229/spark-equivalent-of-if-then-else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39048229/spark-equivalent-of-if-then-else\n",
    "# https://www.kaggle.com/datasets/kartik2112/fraud-detection/discussion/197336\n",
    "\n",
    "concat_data = concat_data.withColumn('hour_binary', F.when(concat_data.trans_hour > 21,1)\n",
    "                                                    .when(concat_data.trans_hour < 4,1)\n",
    "                                                    .otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create age from dob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = concat_data.withColumn('dob_timestamp',F.to_timestamp('dob')) # transform string to datetime\n",
    "\n",
    "# calculates years between dob and transaction date to get age\n",
    "concat_data = concat_data.withColumn('age', F.round(F.datediff(F.col('trans_timestamp'), F.col('dob_timestamp'))/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------------+-----------+-----------------------+\n",
      "| age|total_count|non_fraud_count|fraud_count|within_category_fraud_%|\n",
      "+----+-----------+---------------+-----------+-----------------------+\n",
      "|96.0|        269|            262|          7|                    2.6|\n",
      "|87.0|       4291|           4232|         59|                   1.37|\n",
      "|18.0|       4492|           4434|         58|                   1.29|\n",
      "|77.0|       6093|           6024|         69|                   1.13|\n",
      "|78.0|      10046|           9934|        112|                   1.11|\n",
      "|92.0|       6922|           6845|         77|                   1.11|\n",
      "|63.0|      19159|          18966|        193|                   1.01|\n",
      "|71.0|      13401|          13265|        136|                   1.01|\n",
      "|86.0|       5587|           5537|         50|                   0.89|\n",
      "|80.0|       9553|           9469|         84|                   0.88|\n",
      "+----+-----------+---------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_fraud_cat(concat_data,'age',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although research highlighted above showed the 30-39 age group was most susceptible to fraud, the top 10 in this dataset are mostly over 70 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to drop the following columns as I don't feel they will add anything to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns I don't need\n",
    "cols_to_drop = ['_c0''trans_date_trans_time','merchant','first', 'last','street','city',\\\n",
    "                'state','zip','job','trans_num','dob','a','trans_timestamp','dob_timestamp']\n",
    "\n",
    "concat_data = concat_data.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " _c0                   | 0                   \n",
      " trans_date_trans_time | 2019-01-01 00:00:18 \n",
      " cc_num                | 2703186189652095    \n",
      " category              | misc_net            \n",
      " amt                   | 4.97                \n",
      " gender                | F                   \n",
      " lat                   | 36.0788             \n",
      " long                  | -81.1781            \n",
      " city_pop              | 3495                \n",
      " unix_time             | 1325376018          \n",
      " merch_lat             | 36.011293           \n",
      " merch_long            | -82.048315          \n",
      " is_fraud              | 0                   \n",
      " distance              | 78.59756848823127   \n",
      " trans_day             | Tue                 \n",
      " trans_hour            | 0                   \n",
      " hour_binary           | 1                   \n",
      " age                   | 31.0                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat_data.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- trans_day: string (nullable = true)\n",
      " |-- trans_hour: integer (nullable = true)\n",
      " |-- hour_binary: integer (nullable = false)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation against the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the correlation between the numerical predictor variables and the target variables. I've used this source for reference whilst calculating [[17]](https://www.projectpro.io/recipes/calculate-correlation-pyspark). This shows that 'amt' is the highest correlating feature with the hour_binary column being second highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between each numerical feature and the target \"is_fraud\" class\n",
      "\n",
      "_c0: 0.0005\n",
      "amt: 0.2093\n",
      "lat: 0.0029\n",
      "long: 0.001\n",
      "city_pop: 0.0003\n",
      "unix_time: -0.0133\n",
      "merch_lat: 0.0028\n",
      "merch_long: 0.001\n",
      "distance: 0.0004\n",
      "trans_hour: 0.0132\n",
      "hour_binary: 0.1044\n",
      "age: 0.0107\n"
     ]
    }
   ],
   "source": [
    "## correlation\n",
    "## https://www.projectpro.io/recipes/calculate-correlation-pyspark\n",
    "\n",
    "# assigning empty list\n",
    "num_cols = []\n",
    "\n",
    "schema = concat_data.schema # gets df schema\n",
    "\n",
    "# update lists based on the data types of the columns (from schema)\n",
    "for var in schema.fields:\n",
    "    if isinstance(var.dataType, (IntegerType, DoubleType)):\n",
    "        num_cols.append(var.name)\n",
    "\n",
    "num_cols.remove('is_fraud')\n",
    "print('Correlation between each numerical feature and the target \"is_fraud\" class\\n')\n",
    "for col in num_cols:\n",
    "    corr = concat_data.stat.corr(col,'is_fraud')\n",
    "    print(f'{col}: {round(corr,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the data being imbalanced I can't do a straight random split to create the training and test sets. Instead of used stratified split [[18]](https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark). With this you can set the fraction you want to split the target class by. I've set this to 0.8 for both classes so we get an even proportional split. This creates the training data set, I've then subtracted the training set from the main data frame to get the test data set. In the cell below that I have run the function I created earlier in the analysis to check that the split has been applied correctly and kept the class ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split (stratified)\n",
    "# https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark\n",
    "fractions = {0:0.8,1:0.8} # creates fraction of each to split by - I want even split\n",
    "train_df = concat_data.sampleBy(\"is_fraud\", fractions, seed=42)\n",
    "\n",
    "test_df = concat_data.subtract(train_df) # subtracting a df is like subtracting sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "Number of class 1 (fraudulant records): 7659\n",
      "Number of class 0 (Non-fraudulant records): 1474546\n",
      "99.48326985808306% are in class 0\n",
      "\n",
      "Test Set:\n",
      "Number of class 1 (fraudulant records): 1992\n",
      "Number of class 0 (Non-fraudulant records): 368197\n",
      "99.4618964907115% are in class 0\n"
     ]
    }
   ],
   "source": [
    "# checks atrain/test split using function created above\n",
    "print('Train Set:')\n",
    "fraud_not_fraud_summary(train_df)\n",
    "print('\\nTest Set:')\n",
    "fraud_not_fraud_summary(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under/Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalance of the dataset, the training set will need the instances of target classes balancing out. to test the hypothesis that oversampling will produce better results than undersampling I will apply both techniques to the train_df dataframe so that I can test both methods. I've used code and ideas from this blog post to help with sampling [[19]](https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253)\n",
    "\n",
    "First you find the ratio of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio is 192 to 1\n"
     ]
    }
   ],
   "source": [
    "fraud_df = train_df.filter(F.col('is_fraud') == 1) # filters to fraud\n",
    "not_fraud_df = train_df.filter(F.col('is_fraud') == 0) # filters to not fraud\n",
    "ratio = int(not_fraud_df.count()/fraud_df.count()) # gets ratio\n",
    "print(f'The ratio is {ratio} to 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates oversampled data\n",
    "\n",
    "a = range(ratio)\n",
    "# this duplicates the minority rows\n",
    "oversampled_data = fraud_df.withColumn('dummy', F.explode(F.array([F.lit(x) for x in a]))).drop('dummy') \n",
    "# combine both oversampled minority rows and previous majority rows \n",
    "oversampled_data = not_fraud_df.unionAll(oversampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Set:\n",
      "Number of class 1 (fraudulant records): 1470528\n",
      "Number of class 0 (Non-fraudulant records): 1474546\n",
      "50.068215603410984% are in class 0\n"
     ]
    }
   ],
   "source": [
    "# check the oversampled set\n",
    "print('Oversampled Set:')\n",
    "fraud_not_fraud_summary(oversampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates undersampled data set\n",
    "\n",
    "undersampled_data = not_fraud_df.sample(False, 1/ratio) # creates subset of not fraud using the ratio from above\n",
    "undersampled_data = undersampled_data.unionAll(fraud_df) #joins all the fraud records to the subset of not fraud records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled Set:\n",
      "Number of class 1 (fraudulant records): 7659\n",
      "Number of class 0 (Non-fraudulant records): 7850\n",
      "50.615771487523375% are in class 0\n"
     ]
    }
   ],
   "source": [
    "# check the undersampled set\n",
    "print('Undersampled Set:')\n",
    "fraud_not_fraud_summary(undersampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline\n",
    "\n",
    "I will now put together the pipeline to apply the final transformations of the data to feed into the MLLib models. A pipeline can be used to chain together a number of transformers and estimators to produce a workflow or pipeline of steps to apply to your data [[20]](https://spark.apache.org/docs/1.6.1/ml-guide.html)[[21]](https://www.analyticsvidhya.com/blog/2022/09/implementing-a-machine-learning-pipeline-using-pyspark-library/). The beneifts of using this is once it is set up it can be used repeateldy and each time will replicate the steps in the same order. This pipeline can be applied on the training data to create a model and then used on the test data set by calling .transform() and the same steps including the trained model will be applied.\n",
    "\n",
    "For my pipeline I will be applying these steps:\n",
    "\n",
    "* One hot encoding [[22]](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html)[[23]](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)\n",
    "* Transforming the data into a single vector for input into the model [[24]](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)\n",
    "* scaling the data [[25]](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StandardScaler.html)\n",
    "\n",
    "One hot encoding deals with categorical columns and tranforms them into numerical coded variables that can be input into and interpreted by the machine learning models. To do this I've used a combination of StringIndexer and OneHotEncoder. StringIndexer converts the category strings into numerical indexed labels then OneHotEncoder converts these into binary vectors that the machine learning model can read.\n",
    "\n",
    "The VectorAssembler merges multiple columns into a single vector 'features' column. Most machine learning models in MLLib require this to be done.\n",
    "\n",
    "Finally I've scaled the data as this is a requirement for some machine learning models. In my use case it is a requirement for Logistic regression but is not for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer and Onehotencoding\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/implementing-a-machine-learning-pipeline-using-pyspark-library/\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html\n",
    "# https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html\n",
    "\n",
    "stages = []\n",
    "\n",
    "# creates the indexers and encoders, setting handleInvalid='keep' as this was causing issues with the sample data\n",
    "# as the sample didnt include all categories\n",
    "category_indexer = StringIndexer(inputCol='category',outputCol='category_indexed',handleInvalid='keep')\n",
    "category_encoder = OneHotEncoder(inputCol='category_indexed',outputCol='category_encoded')\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol='gender',outputCol='gender_indexed',handleInvalid='keep')\n",
    "gender_encoder = OneHotEncoder(inputCol='gender_indexed',outputCol='gender_encoded')\n",
    "\n",
    "trans_day_indexer = StringIndexer(inputCol='trans_day',outputCol='trans_day_indexed',handleInvalid='keep')\n",
    "trans_day_encoder = OneHotEncoder(inputCol='trans_day_indexed',outputCol='trans_day_encoded')\n",
    "\n",
    "# adding them to lists to use in the pipeline\n",
    "indexers = [category_indexer,gender_indexer,trans_day_indexer]\n",
    "encoders = [category_encoder,gender_encoder,trans_day_encoder]\n",
    "\n",
    "stages += indexers + encoders # adds indexers and encoders to stages for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler\n",
    "input_cols = ['cc_num','amt','lat','long','city_pop','unix_time','merch_lat',\\\n",
    "              'merch_long','distance','hour_binary','age','category_encoded','gender_encoded','trans_day_encoded']\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalises \n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "stages += [scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_c3ea4f0fff60, StringIndexer_da738e90e5bd, StringIndexer_359e76561c51, OneHotEncoder_97e478afc66e, OneHotEncoder_5cc378249b99, OneHotEncoder_b558e4bc62c8, VectorAssembler_96088e5fba9c, StandardScaler_2affe1ca180e]\n"
     ]
    }
   ],
   "source": [
    "print(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and cross validation with gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was hoping to train 3 machine learning models but due to long running times I had to scale this back and therefore decided not to use Support vector Machines. Fraud detection is a classification problem and the models I will be using are Logistic Regression and a Random Forest classifier. I have decided to use Cross Validation [[26]](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html) in conjunction with ParamGridBuilder [[27]](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html) which is a form of gridsearch that finds the best hyperparameters to use in your model. Due to compute time constraints I also had to tone down the number of folds of the cross validation down to 3 and tune less hyperparameters as it was taking too long to run to be feasible for this project.\n",
    "\n",
    "I have first trained the models on the oversampled data and then the undersampled data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "lr = LogisticRegression(labelCol=\"is_fraud\", featuresCol=\"scaled_features\")\n",
    "\n",
    "# creates a new stages variable appending the model\n",
    "lr_stages = stages + [lr]  # adds the model to stages\n",
    "# pipeline - add stages to pipeline\n",
    "lr_pipeline = Pipeline(stages=lr_stages)\n",
    "\n",
    "# it did have 10 in max iter too\n",
    "# create the parameter grid to search through\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [50, 100]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "# cross validator does k fold cross validation and then fits the best model on the whole of the training data\n",
    "lr_cross_val = CrossValidator(estimator=lr_pipeline,\n",
    "                           estimatorParamMaps=lr_param_grid,\n",
    "                           evaluator=BinaryClassificationEvaluator(labelCol=\"is_fraud\"),\n",
    "                           numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.04 s, sys: 2.38 s, total: 11.4 s\n",
      "Wall time: 41min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit model to data\n",
    "oversampled_lr_cv_model = lr_cross_val.fit(oversampled_data)\n",
    "\n",
    "# Best model\n",
    "oversampled_lr_best_model = oversampled_lr_cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "rf = RandomForestClassifier(labelCol=\"is_fraud\", featuresCol=\"scaled_features\")\n",
    "\n",
    "rf_stages = stages + [rf] # adds the model to stages\n",
    "# pipeline - add staages to pipeline\n",
    "rf_pipeline = Pipeline(stages=rf_stages)\n",
    "\n",
    "# creates the parameter grid to search through\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "# cross validator does k fold cross validation and then fits the best model on the whole of the training data\n",
    "rf_cross_val = CrossValidator(estimator=rf_pipeline,\n",
    "                           estimatorParamMaps=rf_param_grid,\n",
    "                           evaluator=BinaryClassificationEvaluator(labelCol=\"is_fraud\"),\n",
    "                           numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.98 s, sys: 2.99 s, total: 13 s\n",
      "Wall time: 2h 47min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fits model to data\n",
    "oversampled_rf_cv_model = rf_cross_val.fit(oversampled_data)\n",
    "\n",
    "# gets best model\n",
    "oversampled_rf_best_model = oversampled_rf_cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.72 s, sys: 2.3 s, total: 11 s\n",
      "Wall time: 12min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit model to data\n",
    "undersampled_lr_cv_model = lr_cross_val.fit(undersampled_data)\n",
    "\n",
    "# Best model\n",
    "undersampled_lr_best_model = undersampled_lr_cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.08 s, sys: 2.53 s, total: 11.6 s\n",
      "Wall time: 15min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fits model to data\n",
    "undersampled_rf_cv_model = rf_cross_val.fit(undersampled_data)\n",
    "\n",
    "# gets best model\n",
    "undersampled_rf_best_model = undersampled_rf_cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of the models I've used multiple evaluation metrics to ascertain which works best on this dataset.\n",
    "\n",
    "For the best overall performing model I've also extracted the hyperparameters [[28]](https://stackoverflow.com/questions/52498970/how-to-get-the-best-hyperparameter-value-after-crossvalidation-in-pyspark) used and the feature importance [[29]](https://saturncloud.io/blog/understanding-pyspark-random-forest-classifier-feature-importance-with-column-names/)[[30]](https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator)[[31]](https://medium.com/@derekfan/python-sorting-and-its-custom-key-5a22fccc04f8). These will be discussed further in my conclusion.\n",
    "\n",
    "For evaluating the model I have used the hold out test set that was not 'seen' in the training stage and it is transformed using the best model fitted in the gridsearchcv stage. Transformations such as StandardScaler are based on the training set statistics which should prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get evaluation metrics\n",
    "\n",
    "def evaluation(predictions):\n",
    "    \"\"\"\n",
    "    takes the dataframe that has been transformed using the best model after cross validation and returns\n",
    "    various evaluation metrics\n",
    "    \n",
    "    args:\n",
    "        predictions (dataframe): the transformed dataframe\n",
    "        \n",
    "    \n",
    "    returns:\n",
    "        AUC-ROC\n",
    "        AUC-PR\n",
    "        Accuracy\n",
    "        Precision\n",
    "        Recall\n",
    "        F1 Score\n",
    "    \n",
    "    \"\"\"\n",
    "    # evaluators for auc-roc and auc-pr\n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderROC')\n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol='is_fraud', metricName='areaUnderPR')\n",
    "\n",
    "    # uses predictions to calculate auc-roc and auc-pr\n",
    "    auc_roc = evaluator_roc.evaluate(predictions)\n",
    "    auc_pr = evaluator_pr.evaluate(predictions)\n",
    "\n",
    "    print(f'AUC-ROC: {round(auc_roc,4)}')\n",
    "    print(f'AUC-PR: {round(auc_pr,4)}')\n",
    "\n",
    "    # evaluators for accuracy, precision and recall\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='accuracy')\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='weightedPrecision')\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"is_fraud\", metricName='weightedRecall')\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol='is_fraud', metricName='f1')\n",
    "\n",
    "    # Calculate the metric values\n",
    "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "    print(f'Accuracy: {round(accuracy,4)}')\n",
    "    print(f'Precision: {round(precision,4)}')\n",
    "    print(f'Recall: {round(recall,4)}')\n",
    "    print(f'F1 Score: {round(f1,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Logistic Regression model trained with oversampled data:\n",
      "\n",
      "AUC-ROC: 0.9519\n",
      "AUC-PR: 0.2167\n",
      "Accuracy: 0.8634\n",
      "Precision: 0.9942\n",
      "Recall: 0.8634\n",
      "F1 Score: 0.9217\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test set\n",
    "oversampled_lr_predictions = oversampled_lr_best_model.transform(test_df)\n",
    "print('Evaluation of Logistic Regression model trained with oversampled data:\\n')\n",
    "evaluation(oversampled_lr_predictions) # runs evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Random Forest model trained with oversampled data:\n",
      "\n",
      "AUC-ROC: 0.9954\n",
      "AUC-PR: 0.8188\n",
      "Accuracy: 0.9928\n",
      "Precision: 0.9963\n",
      "Recall: 0.9928\n",
      "F1 Score: 0.9941\n",
      "CPU times: user 214 ms, sys: 111 ms, total: 325 ms\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make predictions on the test set\n",
    "oversampled_rf_predictions = oversampled_rf_best_model.transform(test_df)\n",
    "print('Evaluation of Random Forest model trained with oversampled data:\\n')\n",
    "evaluation(oversampled_rf_predictions) # runs evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Logistic Regression model trained with undersampled data:\n",
      "\n",
      "AUC-ROC: 0.9517\n",
      "AUC-PR: 0.2214\n",
      "Accuracy: 0.8615\n",
      "Precision: 0.9942\n",
      "Recall: 0.8615\n",
      "F1 Score: 0.9206\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test set\n",
    "undersampled_lr_predictions = undersampled_lr_best_model.transform(test_df)\n",
    "print('Evaluation of Logistic Regression model trained with undersampled data:\\n')\n",
    "evaluation(undersampled_lr_predictions) # runs evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Random Forest model trained with undersampled data:\n",
      "\n",
      "AUC-ROC: 0.9949\n",
      "AUC-PR: 0.7587\n",
      "Accuracy: 0.9727\n",
      "Precision: 0.9952\n",
      "Recall: 0.9727\n",
      "F1 Score: 0.9822\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test set\n",
    "undersampled_rf_predictions = undersampled_rf_best_model.transform(test_df)\n",
    "print('Evaluation of Random Forest model trained with undersampled data:\\n')\n",
    "evaluation(undersampled_rf_predictions) # runs evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF Parameters:\n",
      "\n",
      "Max Depth: 20\n",
      "Number of Trees: 200\n",
      "Impurity: gini\n"
     ]
    }
   ],
   "source": [
    "# (https://stackoverflow.com/questions/52498970/how-to-get-the-best-hyperparameter-value-after-\n",
    "# crossvalidation-in-pyspark)\n",
    "\n",
    "# extract the model\n",
    "best_rf_model = oversampled_rf_best_model.stages[-1]  # Assuming the last stage is the random forest model\n",
    "\n",
    "# get hyperparameters of the best model\n",
    "best_max_depth = best_rf_model._java_obj.getMaxDepth()\n",
    "best_num_trees = best_rf_model._java_obj.getNumTrees()\n",
    "best_impurity = best_rf_model._java_obj.getImpurity()\n",
    "\n",
    "print(f'Best RF Parameters:\\n')\n",
    "print(f'Max Depth: {best_max_depth}')\n",
    "print(f'Number of Trees: {best_num_trees}')\n",
    "print(f'Impurity: {best_impurity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.5 ms, sys: 50 s, total: 4.55 ms\n",
      "Wall time: 509 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('amt', 0.5356207758104532),\n",
       " ('hour_binary', 0.22565363404917346),\n",
       " ('category_1', 0.021973933768052806),\n",
       " ('category_3', 0.019999205887936196),\n",
       " ('category_0', 0.019259556936518752),\n",
       " ('age', 0.01694829882922136),\n",
       " ('unix_time', 0.014409975948294242),\n",
       " ('city_pop', 0.011161945740649236),\n",
       " ('category_5', 0.010314941669383591),\n",
       " ('category_10', 0.010163776761393553),\n",
       " ('category_2', 0.010105323593246623),\n",
       " ('category_4', 0.010028605614858082),\n",
       " ('cc_num', 0.008000608942503297),\n",
       " ('distance', 0.007684038067793089),\n",
       " ('category_7', 0.00723697527722333)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# https://saturncloud.io/blog/understanding-pyspark-random-forest-classifier-feature-importance-with-column-names/\n",
    "# https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator\n",
    "#https://medium.com/@derekfan/python-sorting-and-its-custom-key-5a22fccc04f8\n",
    "\n",
    "# Extract the Random Forest model from the pipeline\n",
    "rf_model = oversampled_rf_best_model.stages[-1]\n",
    "\n",
    "# create a feature importance instance\n",
    "feature_importance = rf_model.featureImportances\n",
    "\n",
    "# because of the onehotencoding I can't just use the column names from the training data set as these were \n",
    "# transformed into multiple new ones. I need to get the number of categories in each encoder and create feature\n",
    "# names to map to each feature importance\n",
    "\n",
    "num_category_categories = 14  # no. of categories\n",
    "num_gender_categories = 2    # no. of gender categories\n",
    "num_trans_day_categories = 7 # no. of day categories\n",
    "\n",
    "# extends the feature names in relation to the number of categories in the one hot encoded features\n",
    "# makes it category_1, category_2 etc\n",
    "one_hot_feature_names = []\n",
    "for feature in input_cols:\n",
    "    if feature == 'category_encoded':\n",
    "        one_hot_feature_names.extend([f'category_{i}' for i in range(num_category_categories)])\n",
    "    elif feature == 'gender_encoded':\n",
    "        one_hot_feature_names.extend([f'gender_{i}' for i in range(num_gender_categories)])\n",
    "    elif feature == 'trans_day_encoded':\n",
    "        one_hot_feature_names.extend([f'trans_day_{i}' for i in range(num_trans_day_categories)])\n",
    "    else:\n",
    "        one_hot_feature_names.append(feature)\n",
    "\n",
    "# maps feature importances to new feature names\n",
    "feature_importance_with_names = [(one_hot_feature_names[i], feature_importance[i]) \\\n",
    "                                 for i in range(len(feature_importance))]\n",
    "\n",
    "#https://medium.com/@derekfan/python-sorting-and-its-custom-key-5a22fccc04f8\n",
    "sorted_feature_importance = sorted(feature_importance_with_names, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# prints top 15 items\n",
    "sorted_feature_importance[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model using data that hasnt been sampled\n",
    "\n",
    "Although some form of sampling is the suggested method to get the best results when dealing with an imbalanced data set. I thought it would be sensible to benchmark this against running a model on the unsampled data to check the results. As you can see below it actually scored pretty well but not as well as the best Random Forest model on sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Random Forest model with data that was not sampled:\n",
      "\n",
      "AUC-ROC: 0.9667\n",
      "AUC-PR: 0.5863\n",
      "Accuracy: 0.9946\n",
      "Precision: 0.9893\n",
      "Recall: 0.9946\n",
      "F1 Score: 0.9919\n",
      "CPU times: user 165 ms, sys: 41 ms, total: 206 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialise model - uses default parmeters\n",
    "no_sampling_rf = RandomForestClassifier(labelCol=\"is_fraud\",featuresCol=\"scaled_features\")\n",
    "\n",
    "no_sampling_rf_stages = stages + [no_sampling_rf] # adds the model to stages\n",
    "# pipeline - add stages to pipeline\n",
    "no_sampling_rf_pipeline = Pipeline(stages=no_sampling_rf_stages)\n",
    "\n",
    "# fit unsampled train_df\n",
    "no_sampling_rf_model = no_sampling_rf_pipeline.fit(train_df)\n",
    "\n",
    "# test against test set\n",
    "no_sampling_rf_predictions = no_sampling_rf_model.transform(test_df)\n",
    "print('Evaluation of Random Forest model with data that was not sampled:\\n')\n",
    "evaluation(no_sampling_rf_predictions) # runs evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # this stops the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show the best model was the Random Forest classifier trained on the oversampled data set. I used the following metrics for evaluation:\n",
    "\n",
    "* AUC-ROC: 0.9954\n",
    "* AUC-PR: 0.8188\n",
    "* Accuracy: 0.9928\n",
    "* Precision: 0.9963\n",
    "* Recall: 0.9928\n",
    "* F1 Score: 0.9941\n",
    "\n",
    "Often accuracy is the main benchmark used in analysing the performance of models. However, this can be misleading, especially in the case of imbalanced data sets. With this set in particular the majority class (class 0) is c99.5% of the data so predicting all as class 0 would give a model accuracy of 99.5%, but obviously this would not be a good model as no fraud would be caught. Therefore, other metrics give a better picture of the actual effectiveness of the model.\n",
    "\n",
    "Precision, recall and F1 score are all >99% which suggests the model performs well in these areas. Precision relates to the instances of correctly predicted positive class and recall relates to haw many of the actual positives were correctly predicted. A high score like this shows there were low rates of false positives and false negatives. In fraud prediction generally you would be more concerned with minimising false negatives i.e. not catching fraudulent transactions. The F1 score is the harmonic mean between precision and recall. The score of my model suggests this is doing well in relation to these metrics[[32]](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions).\n",
    "\n",
    "These metrics can sometimes also be misleading especially when it comes to imbalanced data, so I've also used AUC-ROC and AUC-PR. AUC-ROC looks at the false positive rate and true positive rate a number closer to 1 suggests the model is able to distinguish well between the positive and negative class. AUC-PR is good for evaluating imbalanced data as it focuses more on the performance in relation to the positive minority class - my model score of 0.82 suggests the model could improve in this area and fails to predict the minority (fraud) class in some instances [[33]](https://medium.com/alliedoffsets/boost-your-binary-classification-game-auc-roc-vs-auc-pr-which-one-should-you-use-28f6518d7bda).\n",
    "\n",
    "Random Forest outperformed Logistic Regression for this problem. This wasn't a surprise as RF is a powerful method that often proves to outperform other models. The oversampled data set also proved to be the best to train the model although not by that far and not by too much, the undersampled RF model was the next best with the AUC-PR for the undersampled model being 0.76. The benchmarked model where the training set was not sampled at all was 0.59 for AUC-PR, this is interesting as it outperforms both Logistic Regression models using the sampled data by a considerable amount. The AUC-PR for the best Logistic Regression model (undersampled) was 0.22.\n",
    "\n",
    "Oversampling although performing best here, has a tendency to overfit because the minority class records are replicated which is likely to have caused the lower AUC-PR. It would have been interesting to also try SMOTE to see if this would get better results. Using the Synthetic Minority Oversampling Technique, rather than replicating the minority class, it uses statistical methods to synthetically create new unique minority records to balance the data and often proves to get good results.\n",
    "\n",
    "Obtaining the feature importance results from the best random forest model gave good insights into which features created a good model. The top features by far were the transaction amount and the time features with age and category following. These were also highlighted when I looked at the correlation between features and the target class before I put the pipeline together. It would be interesting to see if reducing the features thus reducing the noise in the data might get better results.\n",
    "\n",
    "#### Limitations and further work\n",
    "\n",
    "One of the biggest obstacles with this project was the compute time. Because I was using grid search cross validation each model was doing multiple fits and causing the run time to be really long. I initially wanted to test more than two models, but it proved to be taking too much time to run to be realistic. I was hoping to try Support Vector Machines as another model to validate. And also, XGBoost as that tends to get better results than Random Forests but that seemed to require installation on the Lena cluster as it isn't included in Spark MLlib. Using the heavily reduced sample set I've provided with the submission helped with testing but due to the nature of the dataset it didn't work using the sample to get results as they were very different to the full set. I would also have liked to increase the k-folds of the cross validation to at least 5 folds and tuned more parameters within the gridsearch but again this makes it even more compute time expensive as it increases the number of fits needed. I also struggled with the speeds on the Lena cluster. It was often really slow or would completely crash which caused some frustration when I was trying to run the project, it would be good to try this project using a cloud service to see if this reduced the run times. If I was to do this again it would be interesting to try reducing the features down further to try and get better results as the correlation and feature importance suggested this could be the case. Finally, I was hoping to use a confusion matrix as part of the evaluation however, I ran out of time, and it seemed more convoluted than using SKlearn where it's very straightforward. I was reluctant to use anything outside of Pyspark for coding and analysis as the project requirements were to use Pyspark, which was not something I had used before starting this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1.\tCard fraud - credit cards and debit cards combined - worldwide 2014-2021 [Internet]. Statista. [cited 2023 Sep 14]. Available from: https://www.statista.com/statistics/1394119/global-card-fraud-losses/\n",
    " \t \n",
    "2.\tUK Payments Markets Summary 2022 [Internet]. ukfinance.org.uk. 2022 [cited 2023 Sep 14]. Available from: https://www.ukfinance.org.uk/system/files/2022-08/UKF%20Payment%20Markets%20Summary%202022.pdf\n",
    " \t \n",
    "3.\tWikipedia contributors. Credit card fraud [Internet]. Wikipedia, The Free Encyclopedia. 2023 [cited 2023 Sep 14]. Available from: https://en.wikipedia.org/w/index.php?title=Credit_card_fraud&oldid=1173126243\n",
    " \t \n",
    "4.\tAkin J. Identity theft is on the rise, both in incidents and losses [Internet]. Experian.com. Experian; 2022 [cited 2023 Sep 14]. Available from: https://www.experian.com/blogs/ask-experian/identity-theft-statistics/\n",
    " \t \n",
    "5.\tRules-based fraud detection [Internet]. Fraud.net. 2019 [cited 2023 Sep 14]. Available from: https://fraud.net/d/rules-based-fraud-detection/\n",
    " \t \n",
    "6.\tGillis AS. The 5 Vs of big data [Internet]. Data Management. TechTarget; 2021 [cited 2023 Sep 14]. Available from: https://www.techtarget.com/searchdatamanagement/definition/5-Vs-of-big-data\n",
    " \t \n",
    "7.\tShenoy K. Credit card transactions fraud detection dataset [Internet]. 2020 [cited 2023 Sep 14]. Available from: https://www.kaggle.com/datasets/kartik2112/fraud-detection\n",
    " \t \n",
    "8.\tEgan J. Credit card fraud statistics [Internet]. Bankrate. Bankrate.com; 2023 [cited 2023 Sep 14]. Available from: https://www.bankrate.com/finance/credit-cards/credit-card-fraud-statistics/\n",
    " \t \n",
    "9.\tBeheshti N. Random forest classification [Internet]. Towards Data Science. 2022 [cited 2023 Sep 14]. Available from: https://towardsdatascience.com/random-forest-classification-678e551462f5\n",
    " \t \n",
    "10.\tCouronn R, Probst P, Boulesteix A-L. Random forest versus logistic regression: a large-scale benchmark experiment. BMC Bioinformatics [Internet]. 2018;19(1). Available from: http://dx.doi.org/10.1186/s12859-018-2264-5\n",
    " \t \n",
    "11.\tSelecting only numeric or string columns names from PySpark DataFrame [Internet]. GeeksforGeeks. 2021 [cited 2023 Sep 14]. Available from: https://www.geeksforgeeks.org/selecting-only-numeric-or-string-columns-names-from-pyspark-dataframe/\n",
    " \t \n",
    "12.\tWikipedia contributors. Haversine formula [Internet]. Wikipedia, The Free Encyclopedia. 2023 [cited 2023 Sep 14]. Available from: https://en.wikipedia.org/w/index.php?title=Haversine_formula&oldid=1168079505\n",
    " \t \n",
    "13.\tPavlov K. Spherical distance calcualtion based on latitude and longitude with Apache Spark [Internet]. https://gist.github.com/. [cited 2023 Sep 14]. Available from: https://gist.github.com/pavlov99/bd265be244f8a84e291e96c5656ceb5c\n",
    " \t \n",
    "14.\tPySpark to_timestamp()  Convert String to Timestamp type [Internet]. Sparkbyexamples.com. 2023 [cited 2023 Sep 14]. Available from: https://sparkbyexamples.com/spark/pyspark-to_timestamp-convert-string-to-timestamp-type/\n",
    " \t \n",
    "15.\tSpark Timestamp  Extract hour, minute and second [Internet]. Sparkbyexamples.com. 2022 [cited 2023 Sep 14]. Available from: https://sparkbyexamples.com/spark/spark-extract-hour-minute-and-second-from-timestamp/\n",
    " \t \n",
    "16.\tSpark equivalent of IF then ELSE [Internet]. Stack Overflow. [cited 2023 Sep 14]. Available from: https://stackoverflow.com/questions/39048229/spark-equivalent-of-if-then-else\n",
    " \t \n",
    "17.\tHow to calculate correlation in PySpark [Internet]. ProjectPro. [cited 2023 Sep 14]. Available from: https://www.projectpro.io/recipes/calculate-correlation-pyspark\n",
    " \t \n",
    "18.\tStratified sampling with pyspark [Internet]. Stack Overflow. [cited 2023 Sep 14]. Available from: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark\n",
    " \t \n",
    "19.\tWan J. Oversampling and undersampling with PySpark - jun wan [Internet]. Medium. 2020 [cited 2023 Sep 14]. Available from: https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253\n",
    " \t \n",
    "20.\tOverview: estimators, transformers and pipelines - spark.ml - Spark 1.6.1 Documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/1.6.1/ml-guide.html\n",
    " \t \n",
    "21.\tGulati AP. Implementing a machine learning pipeline using PySpark library [Internet]. Analytics Vidhya. 2022 [cited 2023 Sep 14]. Available from: https://www.analyticsvidhya.com/blog/2022/09/implementing-a-machine-learning-pipeline-using-pyspark-library/\n",
    " \t \n",
    "22.\tStringIndexer  PySpark 3.4.1 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html\n",
    " \t \n",
    "23.\tOneHotEncoder  PySpark 3.1.1 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html\n",
    " \t \n",
    "24.\tVectorAssembler  PySpark 3.1.3 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html\n",
    " \t \n",
    "25.\tStandardScaler  PySpark 3.4.1 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StandardScaler.html\n",
    " \t \n",
    "26.\tCrossValidator  PySpark 3.4.1 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html\n",
    " \t \n",
    "27.\tParamGridBuilder  PySpark 3.4.1 documentation [Internet]. Apache.org. [cited 2023 Sep 14]. Available from: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html\n",
    " \t \n",
    "28.\tHow to get the best hyperparameter value after crossvalidation in Pyspark? [Internet]. Stack Overflow. [cited 2023 Sep 14]. Available from: https://stackoverflow.com/questions/52498970/how-to-get-the-best-hyperparameter-value-after-crossvalidation-in-pyspark\n",
    " \t \n",
    "29.\tUnderstanding PySpark Random Forest Classifier feature importance with column names [Internet]. Saturncloud.io. 2023 [cited 2023 Sep 14]. Available from: https://saturncloud.io/blog/understanding-pyspark-random-forest-classifier-feature-importance-with-column-names/\n",
    " \t \n",
    "30.\tFeature selection using feature importance score - creating a PySpark estimator [Internet]. Quasilinear Musings. 2018 [cited 2023 Sep 14]. Available from: https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator\n",
    " \t \n",
    "31.\tFan D. [Python] Sorting and its Custom Key [Internet]. Medium. 2019 [cited 2023 Sep 14]. Available from: https://medium.com/@derekfan/python-sorting-and-its-custom-key-5a22fccc04f8\n",
    " \t \n",
    "32.\tAgrawal SK. Metrics to Evaluate your Classification Model to take the right decisions [Internet]. Analytics Vidhya. 2021 [cited 2023 Sep 14]. Available from: https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions\n",
    " \t \n",
    "33.\tBabbar T. Boost your binary classification game: AUC-ROC vs AUC-PR  which one should you use? [Internet]. AlliedOffsets. 2023 [cited 2023 Sep 14]. Available from: https://medium.com/alliedoffsets/boost-your-binary-classification-game-auc-roc-vs-auc-pr-which-one-should-you-use-28f6518d7bda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
